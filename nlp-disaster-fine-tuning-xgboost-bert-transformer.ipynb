{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score,hamming_loss, roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import pickle\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import RandomSampler, SequentialSampler, DataLoader\n",
    "from torch.utils.data import  Subset, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertModel, BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn import functional as F \n",
    "\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "import re \n",
    "\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2021-10-07T16:15:26.158943Z",
     "iopub.execute_input": "2021-10-07T16:15:26.159217Z",
     "iopub.status.idle": "2021-10-07T16:15:26.181399Z",
     "shell.execute_reply.started": "2021-10-07T16:15:26.159189Z",
     "shell.execute_reply": "2021-10-07T16:15:26.180736Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T17:05:45.321091Z",
     "iopub.execute_input": "2021-10-07T17:05:45.321681Z",
     "iopub.status.idle": "2021-10-07T17:05:45.356439Z",
     "shell.execute_reply.started": "2021-10-07T17:05:45.321646Z",
     "shell.execute_reply": "2021-10-07T17:05:45.355728Z"
    },
    "trusted": true
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T15:33:23.696639Z",
     "iopub.execute_input": "2021-10-07T15:33:23.697177Z",
     "iopub.status.idle": "2021-10-07T15:33:23.716274Z",
     "shell.execute_reply.started": "2021-10-07T15:33:23.697142Z",
     "shell.execute_reply": "2021-10-07T15:33:23.715631Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submission.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T15:33:27.109957Z",
     "iopub.execute_input": "2021-10-07T15:33:27.110226Z",
     "iopub.status.idle": "2021-10-07T15:33:27.118580Z",
     "shell.execute_reply.started": "2021-10-07T15:33:27.110195Z",
     "shell.execute_reply": "2021-10-07T15:33:27.117617Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train.isna().sum()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T15:33:30.867641Z",
     "iopub.execute_input": "2021-10-07T15:33:30.868188Z",
     "iopub.status.idle": "2021-10-07T15:33:30.878650Z",
     "shell.execute_reply.started": "2021-10-07T15:33:30.868153Z",
     "shell.execute_reply": "2021-10-07T15:33:30.877957Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, we have missing values for the locations and keywords.\n",
    "\n",
    "But this will not be very important since we are focusing on the text sentiment whether it s a disaster tweet or not"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets perform some EDA analysis!!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Target count\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.countplot(x='target',data = train)\n",
    "plt.xlabel(\"Target Values\")\n",
    "plt.ylabel(\"Count Values\")\n",
    "sns.despine(left = True, bottom = True)\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T15:33:40.360237Z",
     "iopub.execute_input": "2021-10-07T15:33:40.360801Z",
     "iopub.status.idle": "2021-10-07T15:33:40.525909Z",
     "shell.execute_reply.started": "2021-10-07T15:33:40.360766Z",
     "shell.execute_reply": "2021-10-07T15:33:40.525084Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plottin the most repetitive words in \"text\" column\n",
    "stopwords = set(STOPWORDS)\n",
    "def word_cloud(data, title = None):\n",
    "    cloud = WordCloud(background_color = \"black\",\n",
    "      stopwords = stopwords,\n",
    "      max_words=200,\n",
    "      max_font_size=40, \n",
    "      scale=3,).generate(str(data))\n",
    "    fig = plt.figure(figsize= (8, 6))\n",
    "    plt.axis(\"off\")\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=10)\n",
    "        fig.subplots_adjust(top=0.8)\n",
    "\n",
    "    plt.imshow(cloud)\n",
    "    plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T15:37:29.677434Z",
     "iopub.execute_input": "2021-10-07T15:37:29.678230Z",
     "iopub.status.idle": "2021-10-07T15:37:29.684454Z",
     "shell.execute_reply.started": "2021-10-07T15:37:29.678190Z",
     "shell.execute_reply": "2021-10-07T15:37:29.683519Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Most repeated words in real disaster tweets,\n",
    "#making a word cloud\n",
    "word_cloud(train[train[\"target\"] == 1][\"text\"], \"Most repeated words in real disaster tweets in train data\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T15:37:33.466041Z",
     "iopub.execute_input": "2021-10-07T15:37:33.466791Z",
     "iopub.status.idle": "2021-10-07T15:37:33.922269Z",
     "shell.execute_reply.started": "2021-10-07T15:37:33.466752Z",
     "shell.execute_reply": "2021-10-07T15:37:33.921610Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Words like :\n",
    "- Fire \n",
    "- earthquake\n",
    "- wildfires\n",
    "- Deeds \n",
    "are references to a disaster happening "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets check the words distribution by tweets (disaster/No disaster)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Distribution of keywords in real and fake tweets \n",
    "plt.figure(figsize = (10, 80), dpi = 100)\n",
    "plt.tick_params(axis='x', labelsize=15)\n",
    "plt.tick_params(axis='y', labelsize=12)\n",
    "sns.countplot(y = \"keyword\", hue = \"target\", data = train)\n",
    "plt.legend(loc = 1)\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T15:43:02.911724Z",
     "iopub.execute_input": "2021-10-07T15:43:02.912338Z",
     "iopub.status.idle": "2021-10-07T15:43:06.997396Z",
     "shell.execute_reply.started": "2021-10-07T15:43:02.912302Z",
     "shell.execute_reply": "2021-10-07T15:43:06.996500Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets move on to the intresting part.\n",
    "We are going to build two classifiers: \n",
    "\n",
    "- Ensemble Method: XGBoostClassifier from xgboost in which we are going to use TF-IDF vectorization and fine tune its hyper-parameters in order to get the best model\n",
    "\n",
    "- Transormers: Bert Method from Hugging face library and fine tune its weights with regards to our train process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble Method: XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "for this part we will perform two steps: \n",
    "\n",
    "- First the cleaning which will be a bit harch \n",
    "- Next the fine tuning of the hyper-parameters we will define"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Pre-Processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def text_preprocessing(s):\n",
    "    \"\"\"\n",
    "    - Lowercase the sentence\n",
    "    - Change \"'t\" to \"not\"\n",
    "    - Remove \"@name\"\n",
    "    - Isolate and remove punctuations except \"?\"\n",
    "    - Remove other special characters\n",
    "    - Remove stop words except \"not\" and \"can\"\n",
    "    - Remove trailing whitespace\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Change 't to 'not'\n",
    "    s = re.sub(r\"\\'t\", \" not\", s)\n",
    "    # Remove @name\n",
    "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
    "    # Isolate and remove punctuations except '?'\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
    "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Substitute some non sense words \n",
    "    s = re.sub(r\"\\x89Û_\", \"\", s)\n",
    "    s = re.sub(r\"\\x89ÛÒ\", \"\", s)\n",
    "    s = re.sub(r\"\\x89ÛÓ\", \"\", s)\n",
    "    s = re.sub(r\"\\x89ÛÏWhen\", \"When\",s)\n",
    "    s = re.sub(r\"\\x89ÛÏ\", \"\", s)\n",
    "    s = re.sub(r\"China\\x89Ûªs\", \"China's\", s)\n",
    "    s = re.sub(r\"let\\x89Ûªs\", \"let's\",s)\n",
    "    s = re.sub(r\"\\x89Û÷\", \"\", s)\n",
    "    s = re.sub(r\"\\x89Ûª\", \"\", s)\n",
    "    s = re.sub(r\"\\x89Û\\x9d\", \"\", s)\n",
    "    s = re.sub(r\"å_\", \"\", s)\n",
    "    s = re.sub(r\"\\x89Û¢\", \"\", s)\n",
    "    s = re.sub(r\"\\x89Û¢åÊ\", \"\", s)\n",
    "    s = re.sub(r\"fromåÊwounds\", \"from wounds\",s)\n",
    "    s = re.sub(r\"åÊ\", \"\", s)\n",
    "    s = re.sub(r\"åÈ\", \"\", s)\n",
    "    s = re.sub(r\"JapÌ_n\", \"Japan\", s)    \n",
    "    s = re.sub(r\"Ì©\", \"e\", s)\n",
    "    s = re.sub(r\"å¨\", \"\", s)\n",
    "    s = re.sub(r\"SuruÌ¤\", \"Suruc\", s)\n",
    "    s = re.sub(r\"åÇ\", \"\", s)\n",
    "    s = re.sub(r\"å£3million\", \"3 million\", s)\n",
    "    s = re.sub(r\"åÀ\", \"\", s)\n",
    "    # Remove stopwords except 'not' and 'can'\n",
    "    s = \" \".join([word for word in s.split()\n",
    "                  if word not in stopwords.words('english')\n",
    "                  or word in ['not', 'can']])\n",
    "    # Remove trailing whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T16:13:10.593350Z",
     "iopub.execute_input": "2021-10-07T16:13:10.593771Z",
     "iopub.status.idle": "2021-10-07T16:13:10.606917Z",
     "shell.execute_reply.started": "2021-10-07T16:13:10.593733Z",
     "shell.execute_reply": "2021-10-07T16:13:10.605967Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# apply the function on the text feature\n",
    "train['text']=train['text'].apply(text_preprocessing)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T16:16:11.576639Z",
     "iopub.execute_input": "2021-10-07T16:16:11.576901Z",
     "iopub.status.idle": "2021-10-07T16:16:26.891435Z",
     "shell.execute_reply.started": "2021-10-07T16:16:11.576873Z",
     "shell.execute_reply": "2021-10-07T16:16:26.890649Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting the data: TF-IDF Vectorizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "X = train['text'].values\n",
    "y = train['target'].values\n",
    "  \n",
    "# initializing TfidfVectorizer \n",
    "vectorizar = TfidfVectorizer(max_features=3000, max_df=0.85)\n",
    "# fitting the tf-idf on the given data\n",
    "vectorizar.fit(X)\n",
    "  \n",
    "# splitting the data to training and testing data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "  \n",
    "# transforming the data\n",
    "X_train_tfidf = vectorizar.transform(X_train)\n",
    "X_test_tfidf = vectorizar.transform(X_test)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T16:18:43.241113Z",
     "iopub.execute_input": "2021-10-07T16:18:43.241882Z",
     "iopub.status.idle": "2021-10-07T16:18:43.530844Z",
     "shell.execute_reply.started": "2021-10-07T16:18:43.241845Z",
     "shell.execute_reply": "2021-10-07T16:18:43.530117Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine tuning XGboost hyper-parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "params = {'estimator__learning_rate' :[0.01, 0.1, 0.2],\n",
    "          'estimator__n_estimators':[10,100,500],\n",
    "          'estimator__subsample':(0.2,0.6,0.8),\n",
    "          'estimator__max_depth':np.arange(1,6),\n",
    "          'estimator__min_child_weight':np.arange(1,6)}\n",
    "base_estimator = XGBClassifier()\n",
    "rsearch_cv = RandomizedSearchCV(estimator=base_estimator, param_distributions=params, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "rsearch_cv.fit(X_train_tfidf, y_train)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T16:25:21.769785Z",
     "iopub.execute_input": "2021-10-07T16:25:21.770088Z",
     "iopub.status.idle": "2021-10-07T16:28:07.592354Z",
     "shell.execute_reply.started": "2021-10-07T16:25:21.770059Z",
     "shell.execute_reply": "2021-10-07T16:28:07.591565Z"
    },
    "trusted": true
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Getting the best model out of the randomized search\n",
    "model=rsearch_cv.best_estimator_"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T16:29:01.324016Z",
     "iopub.execute_input": "2021-10-07T16:29:01.324289Z",
     "iopub.status.idle": "2021-10-07T16:29:01.327940Z",
     "shell.execute_reply.started": "2021-10-07T16:29:01.324260Z",
     "shell.execute_reply": "2021-10-07T16:29:01.327230Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#re-fitting the model with the best hyper-parameter we have found yet\n",
    "model.fit(X_train_tfidf, y_train)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T16:30:23.964627Z",
     "iopub.execute_input": "2021-10-07T16:30:23.965450Z",
     "iopub.status.idle": "2021-10-07T16:30:25.547270Z",
     "shell.execute_reply.started": "2021-10-07T16:30:23.965403Z",
     "shell.execute_reply": "2021-10-07T16:30:25.546525Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets evaluate the model on the test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#We first get the predictions and then we compute the accuracy score\n",
    "predictions=model.predict(X_test_tfidf)\n",
    "print(f'accuracy score:{accuracy_score(y_test,predictions)}')\n",
    "print('\\n')\n",
    "print(classification_report(y_test,predictions))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T16:36:13.323292Z",
     "iopub.execute_input": "2021-10-07T16:36:13.323834Z",
     "iopub.status.idle": "2021-10-07T16:36:13.344186Z",
     "shell.execute_reply.started": "2021-10-07T16:36:13.323800Z",
     "shell.execute_reply": "2021-10-07T16:36:13.343649Z"
    },
    "trusted": true
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here it is! we have reached roughly 79% of accuracy on the test set which is a good result.\n",
    "\n",
    "Lets check the transformer method and see what we will get"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformers: BERT& Pytorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "BERT (introduced in this paper) stands for Bidirectional Encoder Representations from Transformers. If you don’t know what most of that means - you’ve come to the right place! Let’s unpack the main ideas:\n",
    "\n",
    "- Bidirectional: to understand the text you’re looking you’ll have to look back (at the previous words) and forward (at the next words)\n",
    "- Transformers: The Attention Is All You Need paper presented the Transformer model. The Transformer reads entire sequences of tokens at once. In a sense, the model is non-directional, while LSTMs read sequentially (left-to-right or right-to-left). The attention mechanism allows for learning contextual relations between words (e.g. his in a sentence refers to Jim).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image.png](attachment:b1abfadf-0078-41ac-9268-6dafa7f55d9b.png)"
   ],
   "metadata": {},
   "attachments": {
    "b1abfadf-0078-41ac-9268-6dafa7f55d9b.png": {
     "image/png": "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"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As yo may noticed in the above sentences: \n",
    "- the word (it) can be related to the animale (sentence 1)\n",
    "- the word (it) is obviosly related to the street (sentence 2) \n",
    "\n",
    "Thanks to the attention mechanism given by the equation above, the word/token (it) won't have the same embedding since it has different meaning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text cleaning for BERT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The level of processing here is much less than in previous approachs because BERT was trained with the entire sentences and needs the whole sequence in order to capture each word meaning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Substitute some non sense words \n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\",text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\",text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\",text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T17:01:07.115580Z",
     "iopub.execute_input": "2021-10-07T17:01:07.116335Z",
     "iopub.status.idle": "2021-10-07T17:01:07.129742Z",
     "shell.execute_reply.started": "2021-10-07T17:01:07.116296Z",
     "shell.execute_reply": "2021-10-07T17:01:07.127612Z"
    },
    "trusted": true
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Applying the cleaning functions to the train and test data\n",
    "train['text']=train['text'].apply(text_preprocessing)\n",
    "test['text']=test['text'].apply(text_preprocessing)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T17:07:07.629403Z",
     "iopub.execute_input": "2021-10-07T17:07:07.629981Z",
     "iopub.status.idle": "2021-10-07T17:07:08.018297Z",
     "shell.execute_reply.started": "2021-10-07T17:07:07.629943Z",
     "shell.execute_reply": "2021-10-07T17:07:08.017554Z"
    },
    "trusted": true
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialising the Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Setting the tokenizer parameters\n",
    "BERT_MODEL_NAME='bert-base-cased'\n",
    "tokeniser=BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T17:08:47.502305Z",
     "iopub.execute_input": "2021-10-07T17:08:47.503015Z",
     "iopub.status.idle": "2021-10-07T17:08:50.973206Z",
     "shell.execute_reply.started": "2021-10-07T17:08:47.502979Z",
     "shell.execute_reply": "2021-10-07T17:08:50.972533Z"
    },
    "trusted": true
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have used the cased version of BERT because we want to capture the meaningful UPPERCASED Words "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting the Input in BERT format "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# First we need to split the training data into train, validation datasets\n",
    "texts, targets = train['text'].values, train['target'].values\n",
    "texts_train, texts_test, targets_train, targets_test = train_test_split(texts, targets, test_size=0.15, random_state=42, stratify=targets)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T17:12:59.476455Z",
     "iopub.execute_input": "2021-10-07T17:12:59.477032Z",
     "iopub.status.idle": "2021-10-07T17:12:59.490305Z",
     "shell.execute_reply.started": "2021-10-07T17:12:59.476993Z",
     "shell.execute_reply": "2021-10-07T17:12:59.489423Z"
    },
    "trusted": true
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we are going to prepare our data in a format that BERT understands: \n",
    "- inputs_ids: the token ids which will be embedded in pytorch tensor with bert dimensions \n",
    "- attention_mask: token encoding with 1 for a token and a 0 for padding \n",
    "- Label: the target for each input\n",
    "\n",
    "All of these parameters will be wrapped inside a Tensor dataset which will be included in a batch dalaloader for trainning and validation datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def get_input(df,labels,train=True):\n",
    "  input_ids=[]\n",
    "  attention_masks=[]\n",
    "  \n",
    "  for sentence in df:\n",
    "    \n",
    "    # we prepare the encoding for each sentence in the dataset\n",
    "    encoding=tokeniser.encode_plus(\n",
    "                    sentence,\n",
    "                    None,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length= 64,\n",
    "                    padding = 'max_length',\n",
    "                    return_token_type_ids= False,\n",
    "                    return_attention_mask= True,\n",
    "                    truncation=True,\n",
    "                    return_tensors ='pt')\n",
    "    \n",
    "    # We gather the tensors in two lists\n",
    "    input_ids.append(encoding['input_ids'])\n",
    "    attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "  #convert inputs to tensors\n",
    "\n",
    "  input_ids=torch.cat(input_ids,dim=0)\n",
    "  attention_masks=torch.cat(attention_masks,dim=0)\n",
    "  labels=torch.tensor(labels)\n",
    "    \n",
    "    \n",
    "  # we wrap the inputs ensemble inside a TensorDataset\n",
    "  dataset=TensorDataset(input_ids,attention_masks,labels)\n",
    "\n",
    "  if train:\n",
    "    sampler=RandomSampler(dataset)\n",
    "  else:\n",
    "    sampler=SequentialSampler(dataset)\n",
    "  \n",
    "  dataloader=DataLoader(dataset,sampler=sampler, batch_size=16)\n",
    "\n",
    "  return dataloader"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T17:24:26.696421Z",
     "iopub.execute_input": "2021-10-07T17:24:26.697178Z",
     "iopub.status.idle": "2021-10-07T17:24:26.705648Z",
     "shell.execute_reply.started": "2021-10-07T17:24:26.697140Z",
     "shell.execute_reply": "2021-10-07T17:24:26.704712Z"
    },
    "trusted": true
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Next we set the batches for the training and validation\n",
    "#the batch size is the recommanded by the 'attention is all you need' paper\n",
    "train_batches=get_input(texts_train,targets_train)\n",
    "val_batches=get_input(texts_test,targets_test,train=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T17:26:08.385366Z",
     "iopub.execute_input": "2021-10-07T17:26:08.385944Z",
     "iopub.status.idle": "2021-10-07T17:26:13.613631Z",
     "shell.execute_reply.started": "2021-10-07T17:26:08.385906Z",
     "shell.execute_reply": "2021-10-07T17:26:13.612877Z"
    },
    "trusted": true
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the BERT-Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are a lot of helpers that make using BERT easy with the Transformers library. Depending on the task you might want to use BertForSequenceClassification, BertForQuestionAnswering or something else.\n",
    "We can choose the **BertForSequenceClassification** for this task. Instead we want to try something else! We’ll use the basic BertModel and build our sentiment classifier on top of it. Let’s load the model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class Bertclf(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    super(Bertclf,self).__init__()\n",
    "    self.bert=BertModel.from_pretrained('bert-base-cased',return_dict=False)\n",
    "    # We are adding a dropout layer\n",
    "    self.drop=nn.Dropout(0.33)\n",
    "    # on top of our bert model we added a Linear model with two hidden layers \n",
    "    self.classifier=torch.nn.Sequential(nn.Linear(768,256),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(256,50),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(50,2))\n",
    "  # the output has two nodes for our binary classification model\n",
    "\n",
    "  def forward(self, input_ids,attention_mask):\n",
    "\n",
    "    last_hidden_state,pooled_output= self.bert(input_ids,attention_mask)\n",
    "    output=self.drop(pooled_output)\n",
    "    output=self.classifier(output)\n",
    "\n",
    "    return output"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T17:36:31.060243Z",
     "iopub.execute_input": "2021-10-07T17:36:31.060544Z",
     "iopub.status.idle": "2021-10-07T17:36:31.067517Z",
     "shell.execute_reply.started": "2021-10-07T17:36:31.060515Z",
     "shell.execute_reply": "2021-10-07T17:36:31.066758Z"
    },
    "trusted": true
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Instantiate the BERT Model & assign it to the GPU\n",
    "model=Bertclf()\n",
    "model.to('cuda')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T17:45:54.491884Z",
     "iopub.execute_input": "2021-10-07T17:45:54.492594Z",
     "iopub.status.idle": "2021-10-07T17:46:28.080003Z",
     "shell.execute_reply.started": "2021-10-07T17:45:54.492559Z",
     "shell.execute_reply": "2021-10-07T17:46:28.078629Z"
    },
    "trusted": true
   },
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# setting the loss function which will be the CrossEntropyLoss\n",
    "loss_fn=nn.CrossEntropyLoss().to('cuda')\n",
    "#Recommended EPOCHS for training in order not to overfit\n",
    "N_EPOCHS=4\n",
    "# optimiser will perform the weights update as its first param\n",
    "#learning rate 5e-5 \n",
    "#epsilon 1e-8\n",
    "optimiser=AdamW(model.parameters(),\n",
    "                lr=5e-5,\n",
    "                eps=1e-08)\n",
    "  \n",
    "total_steps = len(train_batches) * N_EPOCHS\n",
    "# the schedular updates the learning rate each step\n",
    "schedular=get_linear_schedule_with_warmup(optimiser,\n",
    "                                          num_warmup_steps=0,\n",
    "                                          num_training_steps=total_steps)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T17:46:37.587475Z",
     "iopub.execute_input": "2021-10-07T17:46:37.588174Z",
     "iopub.status.idle": "2021-10-07T17:46:37.597114Z",
     "shell.execute_reply.started": "2021-10-07T17:46:37.588136Z",
     "shell.execute_reply": "2021-10-07T17:46:37.596211Z"
    },
    "trusted": true
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper functions for Training/Validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_loop(train_loader,optimizer,Schedular):\n",
    "\n",
    "  #Set the model in training mode  \n",
    "  model.train()\n",
    "  \n",
    "\n",
    "  #We are storing the losses and the good predictions\n",
    "  losses=[]\n",
    "  correct_predicitons=0\n",
    "\n",
    "  for step, batch in enumerate(train_loader):\n",
    "    \n",
    "    #the model should not store the previous gradient \n",
    "    model.zero_grad()\n",
    "\n",
    "\n",
    "    #sending the batches input to the GPU\n",
    "    input,atten,label=(t.to('cuda') for t in batch)\n",
    "    logit=model(input,atten)\n",
    "    loss=loss_fn(logit,label)\n",
    "\n",
    "    #First we apply the softmax on the logits to get the probabilities distribution\n",
    "    #Next the max function give us the max value index\n",
    "    pred=torch.max((F.softmax(logit, dim=1)), dim=1)[1]\n",
    "    correct_predicitons+= torch.sum(pred==label)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "\n",
    "    #we perform the backpropagation on the loss and store in the predicitons\n",
    "    loss.backward()\n",
    "\n",
    "    nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "\n",
    "    #we update the model parameters with the learning rate\n",
    "    optimizer.step()\n",
    "    Schedular.step()\n",
    "\n",
    "  return correct_predicitons/len(texts_train) , np.mean(losses)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T17:54:55.760515Z",
     "iopub.execute_input": "2021-10-07T17:54:55.760941Z",
     "iopub.status.idle": "2021-10-07T17:54:55.774822Z",
     "shell.execute_reply.started": "2021-10-07T17:54:55.760895Z",
     "shell.execute_reply": "2021-10-07T17:54:55.774172Z"
    },
    "trusted": true
   },
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def validation_loop(val_loader,len_input):\n",
    "\n",
    "  #Next we switch to the evaluation mode  \n",
    "  model.eval()\n",
    "\n",
    "  losses=[]\n",
    "  correct_predicitons=0\n",
    "\n",
    "  for batch in (val_loader):\n",
    "\n",
    "      \n",
    "      input,atten,label=(t.to('cuda') for t in batch)\n",
    "    \n",
    "      #No Backpropagation is needed only the feed forward step\n",
    "      with torch.no_grad():\n",
    "        logit=model(input,atten)\n",
    "\n",
    "      loss=loss_fn(logit,label)\n",
    "\n",
    "      pred=torch.max((F.softmax(logit, dim=1)), dim=1)[1]\n",
    "      \n",
    "      correct_predicitons+= torch.sum(pred==label)\n",
    "\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predicitons/len_input , np.mean(losses)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T18:05:11.320448Z",
     "iopub.execute_input": "2021-10-07T18:05:11.320987Z",
     "iopub.status.idle": "2021-10-07T18:05:11.328009Z",
     "shell.execute_reply.started": "2021-10-07T18:05:11.320948Z",
     "shell.execute_reply": "2021-10-07T18:05:11.327238Z"
    },
    "trusted": true
   },
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### the Training/Validation Process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "history=defaultdict(list)\n",
    "\n",
    "best_accuracy=0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "  print(f'Epoch: {epoch+1}/{N_EPOCHS}')\n",
    "  print('-'*10)\n",
    "\n",
    "  train_acc,train_loss=train_loop(train_batches,optimiser,schedular)\n",
    "\n",
    "  print(f'Train accuracy: {train_acc} ; Train loss: {train_loss}')\n",
    "\n",
    "\n",
    "  val_acc, val_loss= validation_loop(val_batches,len(texts_test))\n",
    "  \n",
    "  print(f'val accuracy: {val_acc} ; val loss: {val_loss}')\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), './best_model_state.bin' )\n",
    "    best_accuracy = val_acc"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T18:11:58.542421Z",
     "iopub.execute_input": "2021-10-07T18:11:58.542969Z",
     "iopub.status.idle": "2021-10-07T18:16:01.708953Z",
     "shell.execute_reply.started": "2021-10-07T18:11:58.542929Z",
     "shell.execute_reply": "2021-10-07T18:16:01.708162Z"
    },
    "trusted": true
   },
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, we haven't done any improvment since the first. which is can be concluded with regards to the **val_loss** as it s increasing then stays the same"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "this the input format for the unlabled dataset. the test dataset in our case "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def eval_input(df):\n",
    "  input_ids=[]\n",
    "  attention_masks=[]\n",
    "  \n",
    "  for sentence in df:\n",
    "    \n",
    "    encoding=tokeniser.encode_plus(\n",
    "                    sentence,\n",
    "                    None,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length= 64,\n",
    "                    padding = 'max_length',\n",
    "                    return_token_type_ids= False,\n",
    "                    return_attention_mask= True,\n",
    "                    truncation=True,\n",
    "                    return_tensors ='pt')\n",
    "    \n",
    "    input_ids.append(encoding['input_ids'])\n",
    "    attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "  #convert inputs to tensors\n",
    "\n",
    "  input_ids=torch.cat(input_ids,dim=0)\n",
    "  attention_masks=torch.cat(attention_masks,dim=0)\n",
    "\n",
    "\n",
    "  dataset=TensorDataset(input_ids,attention_masks)\n",
    "\n",
    "  dataloader=DataLoader(dataset,sampler=SequentialSampler(dataset), batch_size=16)\n",
    "\n",
    "  return dataloader"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T18:22:37.855677Z",
     "iopub.execute_input": "2021-10-07T18:22:37.856412Z",
     "iopub.status.idle": "2021-10-07T18:22:37.862973Z",
     "shell.execute_reply.started": "2021-10-07T18:22:37.856377Z",
     "shell.execute_reply": "2021-10-07T18:22:37.862204Z"
    },
    "trusted": true
   },
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#prepare the test batches\n",
    "test_batches=eval_input(test['text'].values)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T18:23:30.207842Z",
     "iopub.execute_input": "2021-10-07T18:23:30.208417Z",
     "iopub.status.idle": "2021-10-07T18:23:32.135677Z",
     "shell.execute_reply.started": "2021-10-07T18:23:30.208381Z",
     "shell.execute_reply": "2021-10-07T18:23:32.134941Z"
    },
    "trusted": true
   },
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to('cuda') for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu()\n",
    "    pred=list(torch.max(probs.detach(),dim=1)[1].numpy())\n",
    "\n",
    "    return pred, probs"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T18:27:24.386877Z",
     "iopub.execute_input": "2021-10-07T18:27:24.387150Z",
     "iopub.status.idle": "2021-10-07T18:27:24.394293Z",
     "shell.execute_reply.started": "2021-10-07T18:27:24.387124Z",
     "shell.execute_reply": "2021-10-07T18:27:24.393309Z"
    },
    "trusted": true
   },
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predictions, probabilities=bert_predict(model,test_batches)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T18:27:31.944307Z",
     "iopub.execute_input": "2021-10-07T18:27:31.944574Z",
     "iopub.status.idle": "2021-10-07T18:27:38.725015Z",
     "shell.execute_reply.started": "2021-10-07T18:27:31.944545Z",
     "shell.execute_reply": "2021-10-07T18:27:38.724261Z"
    },
    "trusted": true
   },
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_predictions_dt=pd.DataFrame({'text':test['text'].values.tolist(),'predictions':predictions})\n",
    "test_predictions_dt.sample(10)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T18:29:58.634395Z",
     "iopub.execute_input": "2021-10-07T18:29:58.634671Z",
     "iopub.status.idle": "2021-10-07T18:29:58.648486Z",
     "shell.execute_reply.started": "2021-10-07T18:29:58.634642Z",
     "shell.execute_reply": "2021-10-07T18:29:58.647558Z"
    },
    "trusted": true
   },
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation on the validation dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "predictions, probabilities=bert_predict(model,val_batches)\n",
    "print(classification_report(list(targets_test), predictions))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T18:31:25.232420Z",
     "iopub.execute_input": "2021-10-07T18:31:25.233142Z",
     "iopub.status.idle": "2021-10-07T18:31:27.620380Z",
     "shell.execute_reply.started": "2021-10-07T18:31:25.233106Z",
     "shell.execute_reply": "2021-10-07T18:31:27.619514Z"
    },
    "trusted": true
   },
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "\n",
    "  plt.figure(figsize=(12,10))\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True sentiment')\n",
    "  plt.xlabel('Predicted sentiment')\n",
    "  plt.show()\n",
    "\n",
    "cm = confusion_matrix(list(targets_test), predictions)\n",
    "df_cm = pd.DataFrame(cm, index=['disaster','no_disaster'], columns=['disaster','no_disaster'])\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-07T18:32:39.095221Z",
     "iopub.execute_input": "2021-10-07T18:32:39.095497Z",
     "iopub.status.idle": "2021-10-07T18:32:39.359270Z",
     "shell.execute_reply.started": "2021-10-07T18:32:39.095469Z",
     "shell.execute_reply": "2021-10-07T18:32:39.358595Z"
    },
    "trusted": true
   },
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}